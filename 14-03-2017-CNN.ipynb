{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9) digits \n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    X = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # Maxpool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 output\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 input, 64 output\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # Fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolution layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # Convolution layer2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connceted layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contruct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  1280 , Minibatch Loss=  25322.269531 , Training Accuracy:  0.33594\n",
      "Iter  2560 , Minibatch Loss=  11544.258789 , Training Accuracy:  0.57812\n",
      "Iter  3840 , Minibatch Loss=  7459.684082 , Training Accuracy:  0.63281\n",
      "Iter  5120 , Minibatch Loss=  6257.607422 , Training Accuracy:  0.75781\n",
      "Iter  6400 , Minibatch Loss=  2846.610352 , Training Accuracy:  0.82812\n",
      "Iter  7680 , Minibatch Loss=  6388.936523 , Training Accuracy:  0.75781\n",
      "Iter  8960 , Minibatch Loss=  2300.992676 , Training Accuracy:  0.88281\n",
      "Iter  10240 , Minibatch Loss=  2353.258545 , Training Accuracy:  0.86719\n",
      "Iter  11520 , Minibatch Loss=  4369.447266 , Training Accuracy:  0.85156\n",
      "Iter  12800 , Minibatch Loss=  1934.122437 , Training Accuracy:  0.88281\n",
      "Iter  14080 , Minibatch Loss=  1602.544434 , Training Accuracy:  0.89062\n",
      "Iter  15360 , Minibatch Loss=  415.852325 , Training Accuracy:  0.96094\n",
      "Iter  16640 , Minibatch Loss=  4285.630859 , Training Accuracy:  0.86719\n",
      "Iter  17920 , Minibatch Loss=  1404.186646 , Training Accuracy:  0.89844\n",
      "Iter  19200 , Minibatch Loss=  2094.772217 , Training Accuracy:  0.89062\n",
      "Iter  20480 , Minibatch Loss=  2516.402832 , Training Accuracy:  0.87500\n",
      "Iter  21760 , Minibatch Loss=  1160.300293 , Training Accuracy:  0.93750\n",
      "Iter  23040 , Minibatch Loss=  2291.861816 , Training Accuracy:  0.90625\n",
      "Iter  24320 , Minibatch Loss=  1957.528564 , Training Accuracy:  0.90625\n",
      "Iter  25600 , Minibatch Loss=  937.445129 , Training Accuracy:  0.93750\n",
      "Iter  26880 , Minibatch Loss=  651.890259 , Training Accuracy:  0.93750\n",
      "Iter  28160 , Minibatch Loss=  1319.268066 , Training Accuracy:  0.92188\n",
      "Iter  29440 , Minibatch Loss=  1317.723267 , Training Accuracy:  0.93750\n",
      "Iter  30720 , Minibatch Loss=  589.653320 , Training Accuracy:  0.96875\n",
      "Iter  32000 , Minibatch Loss=  1192.870361 , Training Accuracy:  0.93750\n",
      "Iter  33280 , Minibatch Loss=  220.200073 , Training Accuracy:  0.99219\n",
      "Iter  34560 , Minibatch Loss=  161.071869 , Training Accuracy:  0.98438\n",
      "Iter  35840 , Minibatch Loss=  2084.193359 , Training Accuracy:  0.92969\n",
      "Iter  37120 , Minibatch Loss=  1677.123413 , Training Accuracy:  0.92969\n",
      "Iter  38400 , Minibatch Loss=  257.314362 , Training Accuracy:  0.97656\n",
      "Iter  39680 , Minibatch Loss=  195.732086 , Training Accuracy:  0.96875\n",
      "Iter  40960 , Minibatch Loss=  285.373779 , Training Accuracy:  0.96875\n",
      "Iter  42240 , Minibatch Loss=  1391.255371 , Training Accuracy:  0.94531\n",
      "Iter  43520 , Minibatch Loss=  1471.261597 , Training Accuracy:  0.91406\n",
      "Iter  44800 , Minibatch Loss=  598.194031 , Training Accuracy:  0.93750\n",
      "Iter  46080 , Minibatch Loss=  246.608200 , Training Accuracy:  0.95312\n",
      "Iter  47360 , Minibatch Loss=  202.009888 , Training Accuracy:  0.97656\n",
      "Iter  48640 , Minibatch Loss=  504.262024 , Training Accuracy:  0.98438\n",
      "Iter  49920 , Minibatch Loss=  332.730835 , Training Accuracy:  0.98438\n",
      "Iter  51200 , Minibatch Loss=  250.594391 , Training Accuracy:  0.97656\n",
      "Iter  52480 , Minibatch Loss=  930.234131 , Training Accuracy:  0.92969\n",
      "Iter  53760 , Minibatch Loss=  45.724228 , Training Accuracy:  0.99219\n",
      "Iter  55040 , Minibatch Loss=  696.914307 , Training Accuracy:  0.94531\n",
      "Iter  56320 , Minibatch Loss=  698.968872 , Training Accuracy:  0.92969\n",
      "Iter  57600 , Minibatch Loss=  770.524963 , Training Accuracy:  0.93750\n",
      "Iter  58880 , Minibatch Loss=  440.452179 , Training Accuracy:  0.96875\n",
      "Iter  60160 , Minibatch Loss=  554.764648 , Training Accuracy:  0.96094\n",
      "Iter  61440 , Minibatch Loss=  232.481735 , Training Accuracy:  0.98438\n",
      "Iter  62720 , Minibatch Loss=  1358.677246 , Training Accuracy:  0.92188\n",
      "Iter  64000 , Minibatch Loss=  456.384918 , Training Accuracy:  0.97656\n",
      "Iter  65280 , Minibatch Loss=  403.671295 , Training Accuracy:  0.96094\n",
      "Iter  66560 , Minibatch Loss=  681.060791 , Training Accuracy:  0.95312\n",
      "Iter  67840 , Minibatch Loss=  208.533554 , Training Accuracy:  0.96875\n",
      "Iter  69120 , Minibatch Loss=  830.985840 , Training Accuracy:  0.92188\n",
      "Iter  70400 , Minibatch Loss=  947.348328 , Training Accuracy:  0.95312\n",
      "Iter  71680 , Minibatch Loss=  306.928253 , Training Accuracy:  0.98438\n",
      "Iter  72960 , Minibatch Loss=  582.329651 , Training Accuracy:  0.97656\n",
      "Iter  74240 , Minibatch Loss=  660.539185 , Training Accuracy:  0.96875\n",
      "Iter  75520 , Minibatch Loss=  628.198303 , Training Accuracy:  0.96094\n",
      "Iter  76800 , Minibatch Loss=  311.842316 , Training Accuracy:  0.98438\n",
      "Iter  78080 , Minibatch Loss=  940.576843 , Training Accuracy:  0.96094\n",
      "Iter  79360 , Minibatch Loss=  245.316147 , Training Accuracy:  0.97656\n",
      "Iter  80640 , Minibatch Loss=  55.513931 , Training Accuracy:  0.97656\n",
      "Iter  81920 , Minibatch Loss=  105.536179 , Training Accuracy:  0.96875\n",
      "Iter  83200 , Minibatch Loss=  901.003052 , Training Accuracy:  0.94531\n",
      "Iter  84480 , Minibatch Loss=  211.106384 , Training Accuracy:  0.96094\n",
      "Iter  85760 , Minibatch Loss=  273.637604 , Training Accuracy:  0.96875\n",
      "Iter  87040 , Minibatch Loss=  413.022003 , Training Accuracy:  0.96094\n",
      "Iter  88320 , Minibatch Loss=  775.495789 , Training Accuracy:  0.96094\n",
      "Iter  89600 , Minibatch Loss=  784.348877 , Training Accuracy:  0.93750\n",
      "Iter  90880 , Minibatch Loss=  637.580933 , Training Accuracy:  0.96094\n",
      "Iter  92160 , Minibatch Loss=  313.892944 , Training Accuracy:  0.98438\n",
      "Iter  93440 , Minibatch Loss=  125.322189 , Training Accuracy:  0.97656\n",
      "Iter  94720 , Minibatch Loss=  272.582855 , Training Accuracy:  0.96875\n",
      "Iter  96000 , Minibatch Loss=  206.380890 , Training Accuracy:  0.99219\n",
      "Iter  97280 , Minibatch Loss=  38.511429 , Training Accuracy:  0.99219\n",
      "Iter  98560 , Minibatch Loss=  532.701782 , Training Accuracy:  0.97656\n",
      "Iter  99840 , Minibatch Loss=  580.111816 , Training Accuracy:  0.94531\n",
      "Iter  101120 , Minibatch Loss=  413.056580 , Training Accuracy:  0.97656\n",
      "Iter  102400 , Minibatch Loss=  956.240356 , Training Accuracy:  0.93750\n",
      "Iter  103680 , Minibatch Loss=  640.441345 , Training Accuracy:  0.97656\n",
      "Iter  104960 , Minibatch Loss=  842.615601 , Training Accuracy:  0.94531\n",
      "Iter  106240 , Minibatch Loss=  939.981689 , Training Accuracy:  0.93750\n",
      "Iter  107520 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  108800 , Minibatch Loss=  501.595581 , Training Accuracy:  0.96094\n",
      "Iter  110080 , Minibatch Loss=  222.044022 , Training Accuracy:  0.98438\n",
      "Iter  111360 , Minibatch Loss=  228.244141 , Training Accuracy:  0.96875\n",
      "Iter  112640 , Minibatch Loss=  363.584229 , Training Accuracy:  0.96094\n",
      "Iter  113920 , Minibatch Loss=  259.085144 , Training Accuracy:  0.98438\n",
      "Iter  115200 , Minibatch Loss=  224.061035 , Training Accuracy:  0.96875\n",
      "Iter  116480 , Minibatch Loss=  636.083679 , Training Accuracy:  0.96094\n",
      "Iter  117760 , Minibatch Loss=  144.526001 , Training Accuracy:  0.97656\n",
      "Iter  119040 , Minibatch Loss=  609.283752 , Training Accuracy:  0.93750\n",
      "Iter  120320 , Minibatch Loss=  466.316864 , Training Accuracy:  0.93750\n",
      "Iter  121600 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  122880 , Minibatch Loss=  535.991211 , Training Accuracy:  0.96094\n",
      "Iter  124160 , Minibatch Loss=  452.102661 , Training Accuracy:  0.95312\n",
      "Iter  125440 , Minibatch Loss=  417.566742 , Training Accuracy:  0.97656\n",
      "Iter  126720 , Minibatch Loss=  278.166840 , Training Accuracy:  0.97656\n",
      "Iter  128000 , Minibatch Loss=  371.774292 , Training Accuracy:  0.94531\n",
      "Iter  129280 , Minibatch Loss=  272.457520 , Training Accuracy:  0.95312\n",
      "Iter  130560 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  131840 , Minibatch Loss=  317.735504 , Training Accuracy:  0.95312\n",
      "Iter  133120 , Minibatch Loss=  356.214905 , Training Accuracy:  0.97656\n",
      "Iter  134400 , Minibatch Loss=  925.523682 , Training Accuracy:  0.94531\n",
      "Iter  135680 , Minibatch Loss=  267.637970 , Training Accuracy:  0.96875\n",
      "Iter  136960 , Minibatch Loss=  207.894775 , Training Accuracy:  0.98438\n",
      "Iter  138240 , Minibatch Loss=  336.774567 , Training Accuracy:  0.97656\n",
      "Iter  139520 , Minibatch Loss=  443.769165 , Training Accuracy:  0.98438\n",
      "Iter  140800 , Minibatch Loss=  381.485535 , Training Accuracy:  0.96094\n",
      "Iter  142080 , Minibatch Loss=  82.246445 , Training Accuracy:  0.98438\n",
      "Iter  143360 , Minibatch Loss=  246.122910 , Training Accuracy:  0.96875\n",
      "Iter  144640 , Minibatch Loss=  14.561264 , Training Accuracy:  0.98438\n",
      "Iter  145920 , Minibatch Loss=  40.935547 , Training Accuracy:  0.98438\n",
      "Iter  147200 , Minibatch Loss=  27.713181 , Training Accuracy:  0.99219\n",
      "Iter  148480 , Minibatch Loss=  16.198807 , Training Accuracy:  0.98438\n",
      "Iter  149760 , Minibatch Loss=  337.224670 , Training Accuracy:  0.96875\n",
      "Iter  151040 , Minibatch Loss=  241.288452 , Training Accuracy:  0.98438\n",
      "Iter  152320 , Minibatch Loss=  554.774414 , Training Accuracy:  0.98438\n",
      "Iter  153600 , Minibatch Loss=  90.530540 , Training Accuracy:  0.96875\n",
      "Iter  154880 , Minibatch Loss=  282.966431 , Training Accuracy:  0.97656\n",
      "Iter  156160 , Minibatch Loss=  185.116501 , Training Accuracy:  0.96094\n",
      "Iter  157440 , Minibatch Loss=  144.582077 , Training Accuracy:  0.96875\n",
      "Iter  158720 , Minibatch Loss=  71.794800 , Training Accuracy:  0.98438\n",
      "Iter  160000 , Minibatch Loss=  247.365936 , Training Accuracy:  0.96875\n",
      "Iter  161280 , Minibatch Loss=  295.886169 , Training Accuracy:  0.95312\n",
      "Iter  162560 , Minibatch Loss=  171.164810 , Training Accuracy:  0.98438\n",
      "Iter  163840 , Minibatch Loss=  26.969971 , Training Accuracy:  0.99219\n",
      "Iter  165120 , Minibatch Loss=  76.885452 , Training Accuracy:  0.97656\n",
      "Iter  166400 , Minibatch Loss=  357.169312 , Training Accuracy:  0.98438\n",
      "Iter  167680 , Minibatch Loss=  86.607559 , Training Accuracy:  0.99219\n",
      "Iter  168960 , Minibatch Loss=  77.754227 , Training Accuracy:  0.98438\n",
      "Iter  170240 , Minibatch Loss=  19.839157 , Training Accuracy:  0.99219\n",
      "Iter  171520 , Minibatch Loss=  94.184952 , Training Accuracy:  0.97656\n",
      "Iter  172800 , Minibatch Loss=  84.061096 , Training Accuracy:  0.98438\n",
      "Iter  174080 , Minibatch Loss=  314.649658 , Training Accuracy:  0.98438\n",
      "Iter  175360 , Minibatch Loss=  139.796356 , Training Accuracy:  0.98438\n",
      "Iter  176640 , Minibatch Loss=  4.435867 , Training Accuracy:  0.99219\n",
      "Iter  177920 , Minibatch Loss=  289.606995 , Training Accuracy:  0.98438\n",
      "Iter  179200 , Minibatch Loss=  170.191513 , Training Accuracy:  0.98438\n",
      "Iter  180480 , Minibatch Loss=  364.880432 , Training Accuracy:  0.97656\n",
      "Iter  181760 , Minibatch Loss=  49.176743 , Training Accuracy:  0.99219\n",
      "Iter  183040 , Minibatch Loss=  47.184822 , Training Accuracy:  0.98438\n",
      "Iter  184320 , Minibatch Loss=  311.232788 , Training Accuracy:  0.96875\n",
      "Iter  185600 , Minibatch Loss=  162.385498 , Training Accuracy:  0.97656\n",
      "Iter  186880 , Minibatch Loss=  191.474121 , Training Accuracy:  0.98438\n",
      "Iter  188160 , Minibatch Loss=  7.266632 , Training Accuracy:  0.99219\n",
      "Iter  189440 , Minibatch Loss=  122.843079 , Training Accuracy:  0.96875\n",
      "Iter  190720 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  192000 , Minibatch Loss=  108.108475 , Training Accuracy:  0.97656\n",
      "Iter  193280 , Minibatch Loss=  373.203064 , Training Accuracy:  0.96875\n",
      "Iter  194560 , Minibatch Loss=  620.568359 , Training Accuracy:  0.92188\n",
      "Iter  195840 , Minibatch Loss=  20.870846 , Training Accuracy:  0.98438\n",
      "Iter  197120 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  198400 , Minibatch Loss=  288.473450 , Training Accuracy:  0.96094\n",
      "Iter  199680 , Minibatch Loss=  234.748032 , Training Accuracy:  0.96875\n",
      " Optimization Finished! \n",
      " Testnig Accuracy:  0.984375\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op(back prop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy \n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print(\"Iter \", str(step*batch_size), \", Minibatch Loss= \", \"{:.6f}\".format(loss), \n",
    "                  \", Training Accuracy: \", \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\" Optimization Finished! \")\n",
    "    \n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\" Testnig Accuracy: \", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256],\n",
    "                                                               keep_prob: 1.}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
