{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_inputs = 784 # MNIST data input (img shape: 28 * 28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrapper for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # Fully connected , 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  1280 , Minibatch Loss=  31262.037109 , Training Accuracy:  0.23438\n",
      "Iter  2560 , Minibatch Loss=  12287.244141 , Training Accuracy:  0.48438\n",
      "Iter  3840 , Minibatch Loss=  7029.283691 , Training Accuracy:  0.71094\n",
      "Iter  5120 , Minibatch Loss=  9319.275391 , Training Accuracy:  0.67969\n",
      "Iter  6400 , Minibatch Loss=  4150.092773 , Training Accuracy:  0.75000\n",
      "Iter  7680 , Minibatch Loss=  3064.134033 , Training Accuracy:  0.82031\n",
      "Iter  8960 , Minibatch Loss=  1885.822266 , Training Accuracy:  0.87500\n",
      "Iter  10240 , Minibatch Loss=  4369.567383 , Training Accuracy:  0.78906\n",
      "Iter  11520 , Minibatch Loss=  2932.252441 , Training Accuracy:  0.85938\n",
      "Iter  12800 , Minibatch Loss=  1929.618652 , Training Accuracy:  0.92969\n",
      "Iter  14080 , Minibatch Loss=  2184.755371 , Training Accuracy:  0.88281\n",
      "Iter  15360 , Minibatch Loss=  1136.101074 , Training Accuracy:  0.87500\n",
      "Iter  16640 , Minibatch Loss=  1310.671753 , Training Accuracy:  0.89844\n",
      "Iter  17920 , Minibatch Loss=  337.121826 , Training Accuracy:  0.94531\n",
      "Iter  19200 , Minibatch Loss=  3954.270752 , Training Accuracy:  0.85938\n",
      "Iter  20480 , Minibatch Loss=  516.006653 , Training Accuracy:  0.96094\n",
      "Iter  21760 , Minibatch Loss=  1165.457886 , Training Accuracy:  0.92969\n",
      "Iter  23040 , Minibatch Loss=  1408.973389 , Training Accuracy:  0.91406\n",
      "Iter  24320 , Minibatch Loss=  986.357544 , Training Accuracy:  0.94531\n",
      "Iter  25600 , Minibatch Loss=  1206.022949 , Training Accuracy:  0.92188\n",
      "Iter  26880 , Minibatch Loss=  2159.506348 , Training Accuracy:  0.89844\n",
      "Iter  28160 , Minibatch Loss=  1266.264282 , Training Accuracy:  0.90625\n",
      "Iter  29440 , Minibatch Loss=  724.162109 , Training Accuracy:  0.93750\n",
      "Iter  30720 , Minibatch Loss=  1672.392090 , Training Accuracy:  0.90625\n",
      "Iter  32000 , Minibatch Loss=  1175.847656 , Training Accuracy:  0.92969\n",
      "Iter  33280 , Minibatch Loss=  576.611572 , Training Accuracy:  0.94531\n",
      "Iter  34560 , Minibatch Loss=  1537.481079 , Training Accuracy:  0.92188\n",
      "Iter  35840 , Minibatch Loss=  567.558167 , Training Accuracy:  0.92188\n",
      "Iter  37120 , Minibatch Loss=  565.982178 , Training Accuracy:  0.96875\n",
      "Iter  38400 , Minibatch Loss=  2295.486816 , Training Accuracy:  0.86719\n",
      "Iter  39680 , Minibatch Loss=  1140.209229 , Training Accuracy:  0.90625\n",
      "Iter  40960 , Minibatch Loss=  471.262817 , Training Accuracy:  0.97656\n",
      "Iter  42240 , Minibatch Loss=  333.100769 , Training Accuracy:  0.95312\n",
      "Iter  43520 , Minibatch Loss=  216.265884 , Training Accuracy:  0.96094\n",
      "Iter  44800 , Minibatch Loss=  862.989563 , Training Accuracy:  0.91406\n",
      "Iter  46080 , Minibatch Loss=  1759.767944 , Training Accuracy:  0.92969\n",
      "Iter  47360 , Minibatch Loss=  941.670105 , Training Accuracy:  0.91406\n",
      "Iter  48640 , Minibatch Loss=  270.996338 , Training Accuracy:  0.96875\n",
      "Iter  49920 , Minibatch Loss=  218.511948 , Training Accuracy:  0.97656\n",
      "Iter  51200 , Minibatch Loss=  230.179398 , Training Accuracy:  0.98438\n",
      "Iter  52480 , Minibatch Loss=  1334.547974 , Training Accuracy:  0.92188\n",
      "Iter  53760 , Minibatch Loss=  493.641907 , Training Accuracy:  0.96875\n",
      "Iter  55040 , Minibatch Loss=  695.075439 , Training Accuracy:  0.94531\n",
      "Iter  56320 , Minibatch Loss=  503.327667 , Training Accuracy:  0.95312\n",
      "Iter  57600 , Minibatch Loss=  416.970154 , Training Accuracy:  0.96094\n",
      "Iter  58880 , Minibatch Loss=  839.484253 , Training Accuracy:  0.92969\n",
      "Iter  60160 , Minibatch Loss=  249.171814 , Training Accuracy:  0.98438\n",
      "Iter  61440 , Minibatch Loss=  1474.126221 , Training Accuracy:  0.91406\n",
      "Iter  62720 , Minibatch Loss=  332.617126 , Training Accuracy:  0.96094\n",
      "Iter  64000 , Minibatch Loss=  244.819366 , Training Accuracy:  0.98438\n",
      "Iter  65280 , Minibatch Loss=  595.742798 , Training Accuracy:  0.93750\n",
      "Iter  66560 , Minibatch Loss=  518.676453 , Training Accuracy:  0.93750\n",
      "Iter  67840 , Minibatch Loss=  1031.990479 , Training Accuracy:  0.94531\n",
      "Iter  69120 , Minibatch Loss=  881.619019 , Training Accuracy:  0.93750\n",
      "Iter  70400 , Minibatch Loss=  606.738586 , Training Accuracy:  0.96875\n",
      "Iter  71680 , Minibatch Loss=  915.707886 , Training Accuracy:  0.94531\n",
      "Iter  72960 , Minibatch Loss=  594.930664 , Training Accuracy:  0.96094\n",
      "Iter  74240 , Minibatch Loss=  509.317352 , Training Accuracy:  0.95312\n",
      "Iter  75520 , Minibatch Loss=  489.194489 , Training Accuracy:  0.96875\n",
      "Iter  76800 , Minibatch Loss=  757.436768 , Training Accuracy:  0.94531\n",
      "Iter  78080 , Minibatch Loss=  658.549683 , Training Accuracy:  0.89844\n",
      "Iter  79360 , Minibatch Loss=  253.276276 , Training Accuracy:  0.97656\n",
      "Iter  80640 , Minibatch Loss=  362.893677 , Training Accuracy:  0.95312\n",
      "Iter  81920 , Minibatch Loss=  276.683350 , Training Accuracy:  0.97656\n",
      "Iter  83200 , Minibatch Loss=  309.344543 , Training Accuracy:  0.96094\n",
      "Iter  84480 , Minibatch Loss=  110.286636 , Training Accuracy:  0.98438\n",
      "Iter  85760 , Minibatch Loss=  778.721558 , Training Accuracy:  0.93750\n",
      "Iter  87040 , Minibatch Loss=  660.820679 , Training Accuracy:  0.94531\n",
      "Iter  88320 , Minibatch Loss=  636.700928 , Training Accuracy:  0.94531\n",
      "Iter  89600 , Minibatch Loss=  659.617737 , Training Accuracy:  0.96094\n",
      "Iter  90880 , Minibatch Loss=  871.547729 , Training Accuracy:  0.95312\n",
      "Iter  92160 , Minibatch Loss=  426.832428 , Training Accuracy:  0.93750\n",
      "Iter  93440 , Minibatch Loss=  835.490967 , Training Accuracy:  0.94531\n",
      "Iter  94720 , Minibatch Loss=  489.313538 , Training Accuracy:  0.95312\n",
      "Iter  96000 , Minibatch Loss=  868.417786 , Training Accuracy:  0.92969\n",
      "Iter  97280 , Minibatch Loss=  379.659607 , Training Accuracy:  0.96094\n",
      "Iter  98560 , Minibatch Loss=  212.705811 , Training Accuracy:  0.96094\n",
      "Iter  99840 , Minibatch Loss=  730.187195 , Training Accuracy:  0.93750\n",
      "Iter  101120 , Minibatch Loss=  194.950958 , Training Accuracy:  0.94531\n",
      "Iter  102400 , Minibatch Loss=  222.736145 , Training Accuracy:  0.98438\n",
      "Iter  103680 , Minibatch Loss=  286.530487 , Training Accuracy:  0.92969\n",
      "Iter  104960 , Minibatch Loss=  378.304626 , Training Accuracy:  0.93750\n",
      "Iter  106240 , Minibatch Loss=  169.511841 , Training Accuracy:  0.96875\n",
      "Iter  107520 , Minibatch Loss=  59.995163 , Training Accuracy:  0.99219\n",
      "Iter  108800 , Minibatch Loss=  235.544586 , Training Accuracy:  0.96875\n",
      "Iter  110080 , Minibatch Loss=  423.343689 , Training Accuracy:  0.97656\n",
      "Iter  111360 , Minibatch Loss=  17.378189 , Training Accuracy:  0.99219\n",
      "Iter  112640 , Minibatch Loss=  584.896362 , Training Accuracy:  0.95312\n",
      "Iter  113920 , Minibatch Loss=  117.681778 , Training Accuracy:  0.98438\n",
      "Iter  115200 , Minibatch Loss=  322.880890 , Training Accuracy:  0.97656\n",
      "Iter  116480 , Minibatch Loss=  1185.358643 , Training Accuracy:  0.95312\n",
      "Iter  117760 , Minibatch Loss=  407.973755 , Training Accuracy:  0.96094\n",
      "Iter  119040 , Minibatch Loss=  75.038452 , Training Accuracy:  0.98438\n",
      "Iter  120320 , Minibatch Loss=  373.181335 , Training Accuracy:  0.96875\n",
      "Iter  121600 , Minibatch Loss=  99.473328 , Training Accuracy:  0.97656\n",
      "Iter  122880 , Minibatch Loss=  191.563705 , Training Accuracy:  0.96875\n",
      "Iter  124160 , Minibatch Loss=  273.735992 , Training Accuracy:  0.96094\n",
      "Iter  125440 , Minibatch Loss=  347.069305 , Training Accuracy:  0.94531\n",
      "Iter  126720 , Minibatch Loss=  141.349304 , Training Accuracy:  0.96875\n",
      "Iter  128000 , Minibatch Loss=  134.642151 , Training Accuracy:  0.97656\n",
      "Iter  129280 , Minibatch Loss=  469.078857 , Training Accuracy:  0.94531\n",
      "Iter  130560 , Minibatch Loss=  371.008118 , Training Accuracy:  0.96094\n",
      "Iter  131840 , Minibatch Loss=  302.940552 , Training Accuracy:  0.96094\n",
      "Iter  133120 , Minibatch Loss=  138.838898 , Training Accuracy:  0.97656\n",
      "Iter  134400 , Minibatch Loss=  914.259460 , Training Accuracy:  0.90625\n",
      "Iter  135680 , Minibatch Loss=  391.114563 , Training Accuracy:  0.95312\n",
      "Iter  136960 , Minibatch Loss=  170.028152 , Training Accuracy:  0.96875\n",
      "Iter  138240 , Minibatch Loss=  415.972107 , Training Accuracy:  0.96875\n",
      "Iter  139520 , Minibatch Loss=  265.153687 , Training Accuracy:  0.96875\n",
      "Iter  140800 , Minibatch Loss=  429.990967 , Training Accuracy:  0.97656\n",
      "Iter  142080 , Minibatch Loss=  49.889160 , Training Accuracy:  0.97656\n",
      "Iter  143360 , Minibatch Loss=  331.247559 , Training Accuracy:  0.96875\n",
      "Iter  144640 , Minibatch Loss=  182.104294 , Training Accuracy:  0.96875\n",
      "Iter  145920 , Minibatch Loss=  325.564758 , Training Accuracy:  0.94531\n",
      "Iter  147200 , Minibatch Loss=  412.684113 , Training Accuracy:  0.96875\n",
      "Iter  148480 , Minibatch Loss=  67.113525 , Training Accuracy:  0.99219\n",
      "Iter  149760 , Minibatch Loss=  201.016968 , Training Accuracy:  0.96875\n",
      "Iter  151040 , Minibatch Loss=  390.737335 , Training Accuracy:  0.96875\n",
      "Iter  152320 , Minibatch Loss=  291.504883 , Training Accuracy:  0.98438\n",
      "Iter  153600 , Minibatch Loss=  180.482895 , Training Accuracy:  0.96875\n",
      "Iter  154880 , Minibatch Loss=  41.717674 , Training Accuracy:  0.97656\n",
      "Iter  156160 , Minibatch Loss=  281.793671 , Training Accuracy:  0.96875\n",
      "Iter  157440 , Minibatch Loss=  362.072174 , Training Accuracy:  0.95312\n",
      "Iter  158720 , Minibatch Loss=  80.116776 , Training Accuracy:  0.98438\n",
      "Iter  160000 , Minibatch Loss=  446.120483 , Training Accuracy:  0.92969\n",
      "Iter  161280 , Minibatch Loss=  304.990723 , Training Accuracy:  0.94531\n",
      "Iter  162560 , Minibatch Loss=  58.479614 , Training Accuracy:  0.99219\n",
      "Iter  163840 , Minibatch Loss=  22.033142 , Training Accuracy:  0.98438\n",
      "Iter  165120 , Minibatch Loss=  163.364243 , Training Accuracy:  0.98438\n",
      "Iter  166400 , Minibatch Loss=  153.170273 , Training Accuracy:  0.96875\n",
      "Iter  167680 , Minibatch Loss=  353.509644 , Training Accuracy:  0.97656\n",
      "Iter  168960 , Minibatch Loss=  145.799408 , Training Accuracy:  0.98438\n",
      "Iter  170240 , Minibatch Loss=  72.240067 , Training Accuracy:  0.98438\n",
      "Iter  171520 , Minibatch Loss=  228.121414 , Training Accuracy:  0.95312\n",
      "Iter  172800 , Minibatch Loss=  89.383514 , Training Accuracy:  0.96875\n",
      "Iter  174080 , Minibatch Loss=  380.477722 , Training Accuracy:  0.96875\n",
      "Iter  175360 , Minibatch Loss=  159.819443 , Training Accuracy:  0.96875\n",
      "Iter  176640 , Minibatch Loss=  24.272522 , Training Accuracy:  0.98438\n",
      "Iter  177920 , Minibatch Loss=  416.594086 , Training Accuracy:  0.96875\n",
      "Iter  179200 , Minibatch Loss=  189.010025 , Training Accuracy:  0.98438\n",
      "Iter  180480 , Minibatch Loss=  217.568863 , Training Accuracy:  0.96875\n",
      "Iter  181760 , Minibatch Loss=  273.547821 , Training Accuracy:  0.97656\n",
      "Iter  183040 , Minibatch Loss=  173.083862 , Training Accuracy:  0.97656\n",
      "Iter  184320 , Minibatch Loss=  364.092041 , Training Accuracy:  0.96875\n",
      "Iter  185600 , Minibatch Loss=  143.765594 , Training Accuracy:  0.97656\n",
      "Iter  186880 , Minibatch Loss=  178.896500 , Training Accuracy:  0.97656\n",
      "Iter  188160 , Minibatch Loss=  66.506767 , Training Accuracy:  0.98438\n",
      "Iter  189440 , Minibatch Loss=  180.237122 , Training Accuracy:  0.97656\n",
      "Iter  190720 , Minibatch Loss=  42.298431 , Training Accuracy:  0.98438\n",
      "Iter  192000 , Minibatch Loss=  57.002472 , Training Accuracy:  0.97656\n",
      "Iter  193280 , Minibatch Loss=  89.053894 , Training Accuracy:  0.97656\n",
      "Iter  194560 , Minibatch Loss=  24.561033 , Training Accuracy:  0.98438\n",
      "Iter  195840 , Minibatch Loss=  56.169495 , Training Accuracy:  0.98438\n",
      "Iter  197120 , Minibatch Loss=  0.000000 , Training Accuracy:  1.00000\n",
      "Iter  198400 , Minibatch Loss=  85.488335 , Training Accuracy:  0.96875\n",
      "Iter  199680 , Minibatch Loss=  1.259323 , Training Accuracy:  0.99219\n",
      " Optimization Finished! \n",
      " Testnig Accuracy:  0.984375\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op(back prop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy \n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print(\"Iter \", str(step*batch_size), \", Minibatch Loss= \", \"{:.6f}\".format(loss), \n",
    "                  \", Training Accuracy: \", \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\" Optimization Finished! \")\n",
    "    \n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\" Testnig Accuracy: \", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256],\n",
    "                                                               keep_prob: 1.}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
